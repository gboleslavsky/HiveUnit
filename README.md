# HiveUnit
I decided to write this tool after I realized that most Hive developers do not write automated unit tests for their queries. After looking at tools that are available for hive 1.xx, I did not find any that are easy to use by developers with SQL background. The fact that Hadoop is almost impossible to install on Windows makes it even more difficult. By a unit test I mean an automated test that developers can run locally before committing to a remote repository, to make sure they have not broken the build. This framework facilitates Test Driven Development of Hive scripts. It does so without any dependencies on local install of any Hadoop software. It is designed to run as a java jar, invoked by shell scripts, one set per OS. It uses Spark 2.XX to run hql, which creates local HDFS behind the scenes. Command level API consists of five commands, and can be trivially extended to support an arbitrary number of commands, each with its own set of parameters. Commands supported so far:

to_hql -- converts test definitions to hql -- given an .xlsx file formatted as shown in test definition spreadsheets in the data folder, generates a folder on the file system, currently named hql, with a set of folders for each spreadsheet, each folder with its own subfolders, one per statement type, of HQL DDL and DML statements, needed to conduct an automated test. Currently Excel .xlsx format is supported, as well csv. Can be trivially extended to support typed csv as well as any typed or untyped tabular data source for input and expected data.

load -- given the hql folder created by eth (previous command), runs CREATE TABLE and INSERT scripts to load input data

hive -- runs script or scripts under test, as configured in the conf sheet of the test definition spreadsheet

assert -- obtains ACTUAL results by running SELECT statements generated by to_hql, compares to EXPECTED results specified in the test definition spreadsheet and generates a report of results.

clean -- runs TRUNCATE statements generated by eth for both input and expected tables to prepare for the next run
